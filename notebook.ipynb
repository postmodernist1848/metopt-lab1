{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import *\n",
    "from funcs import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1, q2, f4\n",
    "# 3 LRS (2 типа h, разные гиперпараметры), Dichotomy, Armijo * 2 (x_0)\n",
    "\n",
    "func = BiFuncStatsDecorator(fopp3)\n",
    "x_0 = np.array([1.2, 1.2])\n",
    "PLOT_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_trajectory(func: BiFunc, trajectory: np.ndarray, title=None):\n",
    "    # Create a meshgrid for the 3D plot\n",
    "    x = np.linspace(-PLOT_SIZE, PLOT_SIZE, 100)\n",
    "    y = np.linspace(-PLOT_SIZE, PLOT_SIZE, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = func(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)  # type: ignore\n",
    "\n",
    "    # Plot the trajectory\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], [func(np.array([x, y]))\n",
    "            for x, y in trajectory], color='r', marker='o')\n",
    "\n",
    "    ax.scatter(trajectory[-1, 0], trajectory[-1, 1], func(trajectory[-1]), color='b', label='Final point')\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')  # type: ignore\n",
    "    #ax.set_zlim(-2, 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "true_minima: Dict[BiFunc, float] = {\n",
    "    q1: -1.9,\n",
    "    q2: 0.0,\n",
    "    f4: -0.514753641275705599276576050856482912233727798097409,\n",
    "    f5: -0.5,\n",
    "    f6: 0,\n",
    "    fopp: -657.573,\n",
    "    fopp2: 0,\n",
    "    fopp3: -0.119789,\n",
    "    fsinsin: -2,\n",
    "}\n",
    "\n",
    "assert func.f in true_minima\n",
    "\n",
    "def print_stats(func: BiFuncStatsDecorator, trajectory: np.ndarray, title=None):\n",
    "\n",
    "    cc, gc, hc = func.call_count, func.gradient_count, func.hessian_count\n",
    "    calculated_min = func(trajectory[-1])\n",
    "    print(\"\\ntitle: \", title)\n",
    "    print(f'Iterations: {len(trajectory) - 1}')\n",
    "    print(f'x: {trajectory[-1]} f(x): {calculated_min}')\n",
    "    print(f'Function evaluations: {cc}')\n",
    "    print(f'Gradient evaluations: {gc}')\n",
    "    print(f'Hessian evaluations: {hc}')\n",
    "    print(f'True minimum: {true_minima[func.f]}')\n",
    "    print(f'Error: {abs(calculated_min - true_minima[func.f])}')\n",
    "    #plot_trajectory(func, trajectory, title)\n",
    "    func.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "from collections import namedtuple\n",
    "\n",
    "xs = [[0.0, 0.0], [1.0, 4.0], [1.0, 1.0]]\n",
    "xs = [np.array(x) for x in xs]\n",
    "\n",
    "h_map = {\n",
    "    constant_h(0.1): \"constant_h(0.1)\",\n",
    "    exponential_decay(0.5): \"exponential_decay(0.5)\",\n",
    "    exponential_decay(0.3): \"exponential_decay(0.3)\"\n",
    "}\n",
    "\n",
    "h_array = h_map.keys()\n",
    "h_name_array = list(h_map.values())\n",
    "func_array = [BiFuncStatsDecorator(q1), BiFuncStatsDecorator(q2), BiFuncStatsDecorator(f4)]\n",
    "\n",
    "algorithms = {\n",
    "    \"Learning rate scheduling\": {\n",
    "        \"algorithm\": learning_rate_scheduling,\n",
    "        \"applier\": lambda args, h: learning_rate_scheduling(args['x_0'], args['func'], h, args['stop_condition'])\n",
    "    },\n",
    "    \"Armijo Gradient Descent\": {\n",
    "        \"algorithm\": steepest_gradient_descent_armijo,\n",
    "        \"applier\": lambda args: steepest_gradient_descent_armijo(args['x_0'], args['func'], args['stop_condition'])\n",
    "    },\n",
    "    \"Dichotomy Gradient Descent\": {\n",
    "        \"algorithm\": steepest_gradient_descent_dichotomy,\n",
    "        \"applier\": lambda args: steepest_gradient_descent_dichotomy(args['x_0'], args['func'], args['eps'], args['stop_condition'])\n",
    "    },\n",
    "    \"Scipy Wolfe Gradient Descent\": {\n",
    "        \"algorithm\": steepest_gradient_descent_scipy_wolfe,\n",
    "        \"applier\": lambda args: steepest_gradient_descent_scipy_wolfe(args['x_0'], args['func'], args['stop_condition'])\n",
    "    },\n",
    "    \"Newton Descent with 1D Search\": {\n",
    "        \"algorithm\": newton_descent_with_1d_search,\n",
    "        \"applier\": lambda args: newton_descent_with_1d_search(args['x_0'], args['func'], args['stop_condition'], armijo_step_selector)\n",
    "    },\n",
    "    \"Dog Leg\": {\n",
    "        \"algorithm\": damped_newton_descent,\n",
    "        \"applier\": lambda args: damped_newton_descent(args['x_0'], args['func'], args['stop_condition'], constant_h(0.1))\n",
    "    }\n",
    "}\n",
    "\n",
    "Stat = namedtuple('Stat', 'func_calls grad_calls trajectory')\n",
    "\n",
    "def get_stat(func, trajectory) -> Stat:\n",
    "    stat: Stat = Stat(func.call_count, func.gradient_count, trajectory)\n",
    "    func.reset()\n",
    "    return stat\n",
    "\n",
    "eps = 1e-9\n",
    "stat_array = []\n",
    "\n",
    "for x_0 in xs:\n",
    "    args = {\n",
    "        \"x_0\": x_0,\n",
    "        \"func\": func,\n",
    "        \"stop_condition\": relative_x_condition(),\n",
    "        \"eps\": eps\n",
    "    }\n",
    "    stats = [[get_stat(func, algorithms[\"Learning rate scheduling\"][\"applier\"](args, h)) for h in h_array]]\n",
    "    algorithms_values = list(algorithms.values())[1:]\n",
    "    for algorithm_info in algorithms_values:\n",
    "        stats.append([get_stat(func, algorithm_info[\"applier\"](args))])\n",
    "    stat_array.append((x_0, func, stats))\n",
    "        \n",
    "alg_name_array = list(algorithms.keys())\n",
    "\n",
    "def print_stat(index: int):\n",
    "    h = \"\"\n",
    "    for stat in stat_array:\n",
    "        x_0 = stat[0]\n",
    "        func: BiFuncStatsDecorator = stat[1]\n",
    "        inner_stat_array = stat[2][index]\n",
    "        for stat_index in range(len(inner_stat_array)):\n",
    "            if (index == 0):\n",
    "                h = h_name_array[stat_index]\n",
    "            func.call_count = inner_stat_array[stat_index].func_calls\n",
    "            func.gradient_count = inner_stat_array[stat_index].grad_calls\n",
    "            print_stats(func, inner_stat_array[stat_index].trajectory, alg_name_array[index] + f' x0={x_0} {h}')\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    \n",
    "# Выводим ответ    \n",
    "for i in range(len(stat_array[0][2])):\n",
    "    print_stat(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = polynomial_decay(0.5, 1)\n",
    "h = geometric_h()\n",
    "h = constant_h(0.01)\n",
    "#h = exponential_decay(0.1)\n",
    "\n",
    "trajectory = learning_rate_scheduling(x_0, func, h, relative_x_condition())\n",
    "print_stats(func, trajectory, \"Learning rate scheduling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "trajectory = steepest_gradient_descent_dichotomy(\n",
    "    x_0, func, eps, relative_x_condition())\n",
    "print_stats(func, trajectory, \"Dichotomy Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = steepest_gradient_descent_armijo(x_0, func, relative_x_condition())\n",
    "print_stats(func, trajectory, \"Armijo Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = steepest_gradient_descent_wolfe(x_0, func, relative_x_condition())\n",
    "print_stats(func, trajectory, \"Wolfe Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = steepest_gradient_descent_scipy_wolfe(x_0, func, relative_x_condition())\n",
    "print_stats(func, trajectory, \"Scipy Wolfe Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = damped_newton_descent(x_0, func, relative_x_condition(), constant_h(0.1))\n",
    "print_stats(func, trajectory, \"Damped Newton Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo_step_selector(k, x, grad, func):\n",
    "    return armijo(x, func, grad)\n",
    "trajectory = newton_descent_with_1d_search(x_0, func, relative_x_condition(), armijo_step_selector)\n",
    "print_stats(func, trajectory, \"Newton method with 1d search (armijo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = bfgs(x_0, func, 1e-3)\n",
    "print_stats(func, trajectory, \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "# Conjugate Gradient Descent\n",
    "fmin_cg(\n",
    "    func,\n",
    "    x_0,\n",
    "    func.gradient,\n",
    "    disp=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
