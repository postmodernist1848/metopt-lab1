{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Quadratic(\n",
    "    np.array([[1, 0], [0, 1]]),\n",
    "    np.array([1, 1]),\n",
    "    -1.4\n",
    ")\n",
    "\n",
    "q2 = Quadratic(\n",
    "    np.array([[0.1, 0], [0, 3]]),\n",
    "    np.array([0, 0]),\n",
    "    0\n",
    ") # Dichotomy is much better\n",
    "\n",
    "q3 = Quadratic(\n",
    "    np.array([[4, 0], [0, 1]]),\n",
    "    np.array([-1, 2]),\n",
    "    1.5\n",
    ")\n",
    "\n",
    "\n",
    "def mf1(x, y):\n",
    "    term1 = np.sin(x) * np.cos(y)\n",
    "    term2 = -1.0 * np.exp(-(x**2 + y**2)/10)\n",
    "    term3 = 0.1 * (x**2 + y**2)\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def mf2(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "\n",
    "def mf3(x, y):\n",
    "    term2 = -1.0 * np.exp(-(x**2 + y**2)/10)\n",
    "    return term2\n",
    "\n",
    "\n",
    "def mf4(x, y):\n",
    "    return (x**2 - 1)**2 + y**2 + 0.5 * x\n",
    "\n",
    "\n",
    "def mf5(x, y):\n",
    "    return (x**2 - 1)**2 + y**2\n",
    "\n",
    "\n",
    "f1 = BiFuncCallableWrapper(mf1)\n",
    "f2 = BiFuncCallableWrapper(mf2)\n",
    "f3 = BiFuncCallableWrapper(mf3)\n",
    "f4 = BiFuncCallableWrapper(mf4) # TOP 1\n",
    "f5 = BiFuncCallableWrapper(mf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = BiFuncStatsDecorator(f4)\n",
    "x_0 = np.array([1.0, 1.0])\n",
    "PLOT_SIZE = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_trajectory(trajectory: np.ndarray, title=None):\n",
    "    # Create a meshgrid for the 3D plot\n",
    "    x = np.linspace(-PLOT_SIZE, PLOT_SIZE, 100)\n",
    "    y = np.linspace(-PLOT_SIZE, PLOT_SIZE, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = func(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)  # type: ignore\n",
    "\n",
    "    # Plot the trajectory\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], [func(np.array([x, y]))\n",
    "            for x, y in trajectory], color='r', marker='o')\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')  # type: ignore\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(func: BiFuncStatsDecorator, trajectory: np.ndarray, title=None):\n",
    "    cc, gc = func.call_count, func.gradient_count\n",
    "    print(f'Iterations: {len(trajectory) - 1}')\n",
    "    print(f'x: {trajectory[-1]} f(x): {func(trajectory[-1])}')\n",
    "    print(f'Function evaluations: {cc}')\n",
    "    print(f'Gradient evaluations: {gc}')\n",
    "    plot_trajectory(trajectory, title)\n",
    "    func.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def constant_h(c: float) -> LearningRateFunc:\n",
    "    return lambda k: c\n",
    "\n",
    "\n",
    "def geometric_h() -> LearningRateFunc:\n",
    "    h0 = 1\n",
    "    return lambda k: h0 / 2**k\n",
    "\n",
    "\n",
    "def exponential_decay(λ: float) -> LearningRateFunc:\n",
    "    assert λ > 0\n",
    "    h0 = 1\n",
    "    return lambda k: h0 * math.exp(-λ * k)\n",
    "\n",
    "\n",
    "def polynomial_decay(α: float, β: float) -> LearningRateFunc:\n",
    "    assert α > 0\n",
    "    assert β > 0\n",
    "    return lambda k: 1/math.sqrt(k + 1) * (β * k + 1) ** -α\n",
    "\n",
    "\n",
    "def relative_x_condition(x: np.ndarray, prev: np.ndarray) -> bool:\n",
    "    # ‖𝑥_{𝑘+1} − 𝑥_𝑘‖ < 𝜀(‖𝑥_{𝑘+1}‖ + 1)\n",
    "    eps = 1e-9\n",
    "    return bool(np.linalg.norm(x - prev) < eps * (np.linalg.norm(x) + 1))\n",
    "\n",
    "\n",
    "def relative_f_condition(x: np.ndarray, prev: np.ndarray) -> bool:\n",
    "    # ‖∇𝑓(𝑥_𝑘)‖^2 < 𝜀‖∇𝑓(𝑥_0)‖^2\n",
    "    eps = 1e-9\n",
    "    return bool(np.linalg.norm(func.gradient(x) ** 2) < eps * np.linalg.norm(func.gradient(x_0)) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = exponential_decay(0.5)\n",
    "h = polynomial_decay(0.5, 1)\n",
    "h = geometric_h()\n",
    "h = constant_h(0.01)\n",
    "\n",
    "trajectory = learning_rate_scheduling(x_0, func, h, relative_x_condition)\n",
    "print_stats(func, trajectory, \"Learning rate scheduling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "trajectory = steepest_gradient_descent_dichotomy(\n",
    "    x_0, func, eps, relative_x_condition)\n",
    "print_stats(func, trajectory, \"Dichotomy Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = steepest_gradient_descent_armijo(x_0, func, relative_x_condition)\n",
    "print_stats(func, trajectory, \"Armijo Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "# Conjugate Gradient Descent, similar to steepest GD\n",
    "fmin_cg(\n",
    "    func,\n",
    "    x_0 - 0.1,\n",
    "    func.gradient,\n",
    "    disp=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
