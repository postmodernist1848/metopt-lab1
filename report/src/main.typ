#let title-page(title:[], body) = {
  set text(font: "Source Sans Pro", size: 14pt)
  set heading(numbering: "1.1.1")
  line(start: (0%, 0%), end: (8.5in, 0%), stroke: (thickness: 2pt))
  align(horizon + left)[
    #text(size: 24pt, title)\
    #v(1em)
    Лабораторная работа №1
    #v(2em)
    Иванов Владимир, M3235,\
    Шепелев Матвей, M3235\
    Зубарев Денис, M3235\
    Команда  ```cpp new m3235::bald[3]```
    
  ]
  
  align(bottom + left)[#datetime.today().display()]
  pagebreak()
  set page(fill: none, margin: auto, numbering: "1")
  align(horizon, outline(indent: auto, title: "Содержание"))
  pagebreak()
  body
}
#show: body => title-page(
  title: [Методы нулевого и первого порядка],
  body
)

#show link: underline

#set figure(supplement: [Рис.])

// #set page(margin: 1.5in) 
= Постановка задачи

В данной работе исследуются методы оптимизации первого порядка, в частности, различные варианты градиентного спуска. 

== Градиентный спуск

Градиентный спуск - способ нахождения минимума функции с помощью движения вдоль ее градиента.

$ x_(k+1) = x_k - h(k)gradient f(x_k) $

Задача лабораторной работы - реализовать несколько видов градиентного спуска и исследовать различия в их эффективности при работе на функциях двух переменных.

= Реализованные методы

== Градиентный спуск с различными стратегиями выбора шага

Нами был самостоятельно реализован спуск с возможностью изменения стратегии выбора шага (где стратегия выбора шага это $h : NN->RR$) и вместе с этим сами стратегии:

1. Константная (в тестовой реализации $h = 0.1$)
2. Экспоненциальная ($h = e^(-lambda k)$, в тестовой реализации $lambda = 0.5, lambda = 0.3$)
//4. Полиномиальная ($h = 1/sqrt(k+1)(beta k + 1)^(-alpha)$, в тестовой реализации $alpha = 1/2, beta = 1$)

Вместе с этим реализованы два критерия остановки:

1. Относительная по координате ($||x_(k+1) - x_k|| < epsilon (||x_(k+1)|| + 1)$) (при измерении результатов пользуемся именно им)
2. Относительная по значению функции ($||gradient f(x_k)||^2 < epsilon ||gradient f(x_0)||^2 $)

Дополнительно есть предохранитель: ограничение на количество операций (10000 в тестовой реализации).

== Градиентный спуск на основе одномерного поиска

Также мы самостоятельно написали наискорейший (steepest) градиентный спуск.

Принцип его работы - ищем $h^*$, такой что значение $f(x_k - h^* gradient f(x_k))$ минимально. Этот $h^*$ ищется с помощью одномерного поиска в направлении антиградиента.

Мы реализовали два вида одномерного поиска:

1. Поиск методом дихотомии (тернарный). Реализован итеративно.
2. Неточный поиск по правилу Армихо (метод backtracking). $c_1, q$ выбираются случайно, $alpha$ - точка пересечения прямой, заданной $c_1$, и оси $O X$.

Градиент квадратичной функции вычисляется аналитически, для не квадратичной используем приближенную симметричную производную.
$ f'_i (x) = (f_i (x + epsilon) - f_i (x - epsilon))/(2 epsilon), i in {0, 1} $
Если градиент равен 0, то используется случайное смещение на $epsilon$.


// Для сравнения был использован Nonlinear Conjugate Gradient из библиотеки Scipy (```python scipy.optimize.fmin_cg(...)```). В отличие от steepest descent, этот алгоритм не просто движется в направлении антиградиента, а сдвигает его на прошлое направление, умноженное на некоторый $beta$, который вычисляется из предыдущих антиградиентов.

Также для сравнения мы взяли поиск на основе сильного условия Вульфе из библиотеки Scipy (```python scipy.optimize.line_search```). У него есть одна особенность - если поиск не сходится, то он возвращает ```python None```. В таком случае мы запускаем наш поиск методом backtracking.
#pagebreak()
= Результаты работы методов

Эффективность методов исследуем на следующих функциях:

== Квадратичная функция $q_1$
$ q_1 = (A = mat(1, 0; 0, 1), B = mat(1;1), C = 1.4) $

#table(
  columns: (auto, auto, auto, auto, auto, auto),
  align: center + horizon,
  [*метод*], [*x0*], [*итераций*], [*вычислений функции*], [*вычислений градиента*], [*погрешность*],
  
  [LR const(0.1)], [[0,0]], [73], [0], [73], [5.77e-15],
  [LR exp(0.5)], [[0,0]], [34], [0], [34], [1.07e-4],
  [LR exp(0.3)], [[0,0]], [44], [0], [44], [1.09e-7],
  
  [LR const(0.1)], [[1,4]], [10002], [0], [10002], [1.33e-15],
  [LR exp(0.5)], [[1,4]], [38], [0], [38], [4.82e-3],
  [LR exp(0.3)], [[1,4]], [51], [0], [51], [4.89e-6],
  
  [LR const(0.1)], [[1,1]], [10002], [0], [10002], [8.88e-16],
  [LR exp(0.5)], [[1,1]], [36], [0], [36], [9.64e-4],
  [LR exp(0.3)], [[1,1]], [48], [0], [48], [9.79e-7],
  
  [Armijo GD], [[0,0]], [20], [1614], [20], [7.99e-15],
  [Armijo GD], [[1,4]], [19], [20835], [19], [1.78e-15],
  [Armijo GD], [[1,1]], [18], [21332], [18], [4.44e-15],
  
  [Dichotomy GD], [[0,0]], [14], [843], [14], [2.22e-16],
  [Dichotomy GD], [[1,4]], [250], [15031], [250], [1.33e-15],
  [Dichotomy GD], [[1,1]], [344], [20663], [344], [4.44e-16],
  
  [Scipy Wolfe GD], [[0,0]], [2], [20017], [5], [0.0],
  [Scipy Wolfe GD], [[1,4]], [2], [20017], [5], [0.0],
  [Scipy Wolfe GD], [[1,1]], [2], [20017], [5], [0.0],
)


Максимально стандартная, хорошо обусловленная функция с единственным глобальным минимум в точке (0, 0). Результаты не очень интересные: почти все алгоритмы управились примерно за одно количество итераций (за исключением градиентного спуска с константным шагом). Спуску на основе одномерного поиска потребовалось заметно больше вычислений функций, но в то же время в среднем он был точнее поиска с планированием шага.

#figure(
  image("q1 lr constant.png"),
  caption: [ГС с константным шагом],
)

#figure(image("q1 sd armijo.png"),
caption: [ГС на основе дихотомии])

#pagebreak()

== Квадратичная функция $q_2$:
$ q_2 = (A = mat(0.1, 0; 0, 3), B = mat(0, 0), C = 0.0) $

#table(
  columns: (auto, auto, auto, auto, auto, auto),
  align: center + horizon,
  [*метод*], [*x0*], [*итераций*], [*вычислений функции*], [*вычислений градиента*], [*погрешность*],
  
  // LR scheduling data for quadratic function (from previous table)
  [LR const(0.1)], [[1,4]], [721], [0], [721], [2.89e-14],
  [LR exp(0.5)], [[1,4]], [45], [0], [45], [2.39],
  [LR exp(0.3)], [[1,4]], [62], [0], [62], [1.97e-2],
  
  [LR const(0.1)], [[1,1]], [719], [0], [719], [2.63e-14],
  [LR exp(0.5)], [[1,1]], [43], [0], [43], [1.81e-1],
  [LR exp(0.3)], [[1,1]], [61], [0], [61], [1.94e-2],
  
  // Armijo data
  [Armijo GD], [[1,4]], [1315], [4407], [1315], [1.95e-12],
  [Armijo GD], [[1,1]], [1289], [24477], [1289], [2.04e-12],
  
  // Dichotomy data
  [Dichotomy GD], [[1,4]], [18], [1004], [18], [1.12e-14],
  [Dichotomy GD], [[1,1]], [14], [799], [14], [3.05e-15],
  
  // Scipy Wolfe data
  [Scipy Wolfe GD], [[1,4]], [123], [20328], [369], [5.91e-15],
  [Scipy Wolfe GD], [[1,1]], [194], [20587], [583], [2.26e-14],
)

Унимодальная функция с плохой обусловленностью (condition number = 30). Разброс в результатах тут побольше.
#pagebreak()
Плохо показал себя спуск на основе backtracking: из-за неточности поиска по правилу Армихо (и, возможно, неудачно выбранных $c_1, q$) ему потребовалось достаточно много итераций.

#figure(image("image (2).png"), caption: [ГС с поиском на основе backtracking])

#pagebreak()
С другой стороны, особенно сильно тут засиял ГС на основе дихотомии: он смог максимально быстро найти точный минимум, что неудивительно, ведь эта функция - фактически идеальный юзкейс для тернарного поиска.

#figure(image("image.png"), caption: [график ГС на основе дихотомии])

#pagebreak()
Сильно оплошал градиентный спуск с экспоненциальным шагом ($lambda = 0.5$). Он не сумел найти минимум и в целом пошел куда-то не туда.

#figure(image("image (1).png"), caption: [ГС на основе экспонецниального выбора шага])

#pagebreak()

== Мультимодальная функция $f_4$
$ f_4 = (x^2 - 1)^2 + y^2 + 0.5x $

#table(
  columns: (auto, auto, auto, auto, auto, auto),
  align: center + horizon,
  [*метод*], [*x0*], [*итераций*], [*вычислений функции*], [*вычислений градиента*], [*погрешность*],
  
  [LR const(0.1)], [[0,0]], [1295], [0], [1295], [9.07e-13],
  [LR exp(0.5)], [[0,0]], [8], [0], [8], [2.80e+117],
  [LR exp(0.3)], [[0,0]], [7], [0], [7], [2.14e+118],
  
  [LR const(0.1)], [[1,4]], [1980], [0], [1980], [9.98e-1],
  [LR exp(0.5)], [[1,4]], [38], [0], [38], [1.00e+0],
  [LR exp(0.3)], [[1,4]], [50], [0], [50], [3.48e-6],
  
  [LR const(0.1)], [[1,1]], [7036], [0], [7036], [9.98e-1],
  [LR exp(0.5)], [[1,1]], [36], [0], [36], [9.99e-1],
  [LR exp(0.3)], [[1,1]], [45], [0], [45], [2.19e-7],
  
  [Armijo GD], [[0,0]], [11], [20609], [11], [3.11e-12],
  [Armijo GD], [[1,4]], [36], [21746], [36], [1.37e-11],
  [Armijo GD], [[1,1]], [45], [23365], [45], [5.13e-13],
  
  [Dichotomy GD], [[0,0]], [2], [113], [2], [1.11e-16],
  [Dichotomy GD], [[1,4]], [10], [569], [10], [9.98e-1],
  [Dichotomy GD], [[1,1]], [13], [745], [13], [9.98e-1],
  
  [Scipy Wolfe GD], [[0,0]], [4], [20027], [13], [5.83e-13],
  [Scipy Wolfe GD], [[1,4]], [28], [20134], [83], [1.11e-16],
  [Scipy Wolfe GD], [[1,1]], [25], [20134], [73], [4.23e-13],
)

Мультимодальная функция с глобальным минимумом в точке $ x_0 approx (-1.058, 0), space f(x_0) approx -0.515 $ и еще одним локальным в точке \ $ x_1 approx (0.930,0), space f(x_1)approx 0.483 $Градиентный спуск на основе дихотомии в двух из трех случаев нашел неверный минимум (неудивительно, ведь тернарный поиск работает корректно только с унимодальными функциями).

#figure(image("f4 dichotomy.png"), caption: [мимо :(])

И поиск на основе правила Армихо, и на основе условий Вульфа смогли найти глобальный минимум с достаточно малой погрешностью, однако для этого им потребовалось достаточно много значений функции.

#figure(
image("f4 armijo.png"), caption: [в точку :)]
)

Среди ГС со стратегией выбора шага с задачей не справился ни один. 

#figure(image("f4 exp.png"), caption: [???????????? (экспоненциальная стратегия)])

== Зашумленная $f_4$.

Для зашумления существующих функций использовался следующий класс с кастомизируемым фактором шума (0.3 в тестовой реализации):

```python
class NoisyWrapper:
    f: BiFunc
    factor: float

    def __init__(self, f: BiFunc, factor: float = 1.0):
        self.f = f
        self.factor = factor

    def __call__(self, x: np.ndarray):
        return self.f.__call__(x) + random.random() * self.factor

    def gradient(self, x: np.ndarray):
        return self.f.gradient(x)
```
#table(
  columns: (auto, auto, auto, auto, auto, auto),
  align: center + horizon,
  [*метод*], [*x0*], [*итераций*], [*вычислений функции*], [*вычислений градиента*], [*погрешность*],
  
  // LR scheduling data
  [LR const(0.1)], [[0,0]], [1238], [0], [1238], [6.92e-2],
  [LR exp(0.5)], [[0,0]], [8], [0], [8], [2.80e+117],
  [LR exp(0.3)], [[0,0]], [7], [0], [7], [2.14e+118],
  
  [LR const(0.1)], [[1,4]], [83], [0], [83], [1.06],
  [LR exp(0.5)], [[1,4]], [38], [0], [38], [1.09],
  [LR exp(0.3)], [[1,4]], [50], [0], [50], [2.31e-1],
  
  [LR const(0.1)], [[1,1]], [5300], [0], [5300], [1.28],
  [LR exp(0.5)], [[1,1]], [36], [0], [36], [1.07],
  [LR exp(0.3)], [[1,1]], [45], [0], [45], [2.39e-1],
  
  // Armijo data
  [Armijo GD], [[0,0]], [895], [14935], [895], [1.93e-1],
  [Armijo GD], [[1,4]], [4081], [67705], [4081], [3.28e-1],
  [Armijo GD], [[1,1]], [314], [5234], [314], [1.27e-1],
  
  // Dichotomy data
  [Dichotomy GD], [[0,0]], [10002], [608011], [10002], [8.17e-2],
  [Dichotomy GD], [[1,4]], [10002], [608011], [10002], [1.76e-1],
  [Dichotomy GD], [[1,1]], [10002], [608359], [10002], [2.62e-1],
  
  // Scipy Wolfe data
  [Scipy Wolfe GD], [[0,0]], [1880], [37471], [6543], [1.66e-1],
  [Scipy Wolfe GD], [[1,4]], [2148], [43464], [7490], [7.39e-2],
  [Scipy Wolfe GD], [[1,1]], [7222], [146795], [25245], [1.92e-1],
)

Все ГС с выбором шага не сумели найти глобальный минимум и/или улетели очень далеко. ГС на основе дихотомии не смогла сойтись и вышла по предохранителю (опять же, неудивительно), но все же приблизилась к минимуму. ГС на основе Армихо/Вульфа смогли достаточно близко приблизиться к глобальному минимуму, однако Scipy Wolfe потребовалось больше итераций и вычислений значений функций при старте из $x_0 = (1,1)$

#figure(image("noisy f4.png"), caption: [зашумленная функция])

= Вывод

В результате лабораторной работы мы реализовали несколько методов градиентного спуска и проанализировали их эффективность на нескольких функциях.

Наименее эффективен градиентный спуск с постоянным шагом, т.к он требует наибольшее число итераций. ГС с экспоненциальным шагом нередко не только не находит минимум, но и выходит за пределы допустимой области, так что часто он просто не работает.

ГС на основе дихотомии прекрасно работает на унимодальных функциях, но неэффективен на мультимодальных. ГС на основе правила Вульфа из Scipy - самый точный из всех исследуемых, но в некоторых случаях требует больше вычислений.

Наиболее эффективным и универсальным оказался поиск на основе правила Армихо, обеспечивающий максимальный баланс между точностью и скоростью работы.

При этом важно заметить, что эффективность работы методов сильно зависит от функции: так, на хорошо обусловленной унимодальной функции все методы отработали примерно одинаково, но на других уже стали проявляться различия. Наиболее устойчивыми оказались ГС на основе правила Армихо и условий Вульфа -- во всех случаях они смогли найти минимум с достаточной точностью.  

= Приложения

Исходный код можно найти на: _#link("https://github.com/postmodernist1848/metopt-lab1")_
// // --------------------------

// #pagebreak()

// q1:
// #table(
//   columns: (auto, auto, auto, auto, auto, auto),
//   align: center + horizon,
//   [*Method*], [*x0*], [*Iterations*], [*Function Evals*], [*Gradient Evals*], [*Error*],
  
//   // LR scheduling data
//   [LR const(0.1)], [[0,0]], [73], [0], [73], [5.77e-15],
//   [LR exp(0.5)], [[0,0]], [34], [0], [34], [1.07e-4],
//   [LR exp(0.3)], [[0,0]], [44], [0], [44], [1.09e-7],
  
//   [LR const(0.1)], [[1,4]], [10002], [0], [10002], [1.33e-15],
//   [LR exp(0.5)], [[1,4]], [38], [0], [38], [4.82e-3],
//   [LR exp(0.3)], [[1,4]], [51], [0], [51], [4.89e-6],
  
//   [LR const(0.1)], [[1,1]], [10002], [0], [10002], [8.88e-16],
//   [LR exp(0.5)], [[1,1]], [36], [0], [36], [9.64e-4],
//   [LR exp(0.3)], [[1,1]], [48], [0], [48], [9.79e-7],
  
//   // Armijo data
//   [Armijo GD], [[0,0]], [20], [1614], [20], [7.99e-15],
//   [Armijo GD], [[1,4]], [19], [20835], [19], [1.78e-15],
//   [Armijo GD], [[1,1]], [18], [21332], [18], [4.44e-15],
  
//   // Dichotomy data
//   [Dichotomy GD], [[0,0]], [14], [843], [14], [2.22e-16],
//   [Dichotomy GD], [[1,4]], [250], [15031], [250], [1.33e-15],
//   [Dichotomy GD], [[1,1]], [344], [20663], [344], [4.44e-16],
  
//   // Scipy Wolfe data
//   [Scipy Wolfe GD], [[0,0]], [2], [20017], [5], [0.0],
//   [Scipy Wolfe GD], [[1,4]], [2], [20017], [5], [0.0],
//   [Scipy Wolfe GD], [[1,1]], [2], [20017], [5], [0.0],
// )

// #pagebreak()
// q2:
// #table(
//   columns: (auto, auto, auto, auto, auto, auto),
//   align: center + horizon,
//   [*Method*], [*x0*], [*Iterations*], [*Function Evals*], [*Gradient Evals*], [*Error*],
  
//   // LR scheduling data for quadratic function (from previous table)
//   [LR const(0.1)], [[1,4]], [721], [0], [721], [2.89e-14],
//   [LR exp(0.5)], [[1,4]], [45], [0], [45], [2.39],
//   [LR exp(0.3)], [[1,4]], [62], [0], [62], [1.97e-2],
  
//   [LR const(0.1)], [[1,1]], [719], [0], [719], [2.63e-14],
//   [LR exp(0.5)], [[1,1]], [43], [0], [43], [1.81e-1],
//   [LR exp(0.3)], [[1,1]], [61], [0], [61], [1.94e-2],
  
//   // Armijo data
//   [Armijo GD], [[1,4]], [1315], [4407], [1315], [1.95e-12],
//   [Armijo GD], [[1,1]], [1289], [24477], [1289], [2.04e-12],
  
//   // Dichotomy data
//   [Dichotomy GD], [[1,4]], [18], [1004], [18], [1.12e-14],
//   [Dichotomy GD], [[1,1]], [14], [799], [14], [3.05e-15],
  
//   // Scipy Wolfe data
//   [Scipy Wolfe GD], [[1,4]], [123], [20328], [369], [5.91e-15],
//   [Scipy Wolfe GD], [[1,1]], [194], [20587], [583], [2.26e-14],
// )

// #pagebreak()
// f4:
// #table(
//   columns: (auto, auto, auto, auto, auto),
//   inset: 8pt,
//   align: center,
//   [*Starting Point*], [*Method*], [*Iterations*], [*Gradient Evals*], [*Error*],
  
//   // x0=[0,0]
//   [[0,0]], [LR scheduling const(0.1)], [2132], [2132], [9.97e-13],
//   [[0,0]], [LR scheduling exp(0.5)], [8], [8], [2.80e+117],
//   [[0,0]], [LR scheduling exp(0.3)], [7], [7], [2.14e+118],
//   [[0,0]], [Armijo], [17], [17], [5.23e-13],
//   [[0,0]], [Dichotomy], [3], [3], [1.11e-16],
//   [[0,0]], [Scipy Wolfe], [4], [13], [5.83e-13],
  
//   // x0=[1,4]
//   [[1,4]], [LR scheduling const(0.1)], [3422], [3422], [9.98e-01],
//   [[1,4]], [LR scheduling exp(0.5)], [38], [38], [1.00],
//   [[1,4]], [LR scheduling exp(0.3)], [50], [50], [3.48e-06],
//   [[1,4]], [Armijo], [40], [40], [7.79e-13],
//   [[1,4]], [Dichotomy], [11], [11], [9.98e-01],
//   [[1,4]], [Scipy Wolfe], [30], [90], [5.77e-15],
  
//   // x0=[1,1]
//   [[1,1]], [LR scheduling const(0.1)], [1742], [1742], [9.98e-01],
//   [[1,1]], [LR scheduling exp(0.5)], [36], [36], [9.99e-01],
//   [[1,1]], [LR scheduling exp(0.3)], [45], [45], [2.19e-07],
//   [[1,1]], [Armijo], [40], [40], [1.08e-12],
//   [[1,1]], [Dichotomy], [13], [13], [9.98e-01],
//   [[1,1]], [Scipy Wolfe], [28], [82], [1.22e-12],
// )


// #pagebreak()
// шум f4
// #table(
//   columns: (auto, auto, auto, auto, auto, auto),
//   align: center + horizon,
//   [*Method*], [*x0*], [*Iterations*], [*Function Evals*], [*Gradient Evals*], [*Error*],
  
//   // LR scheduling data
//   [LR const(0.1)], [[0,0]], [1238], [0], [1238], [6.92e-2],
//   [LR exp(0.5)], [[0,0]], [8], [0], [8], [2.80e+117],
//   [LR exp(0.3)], [[0,0]], [7], [0], [7], [2.14e+118],
  
//   [LR const(0.1)], [[1,4]], [83], [0], [83], [1.06],
//   [LR exp(0.5)], [[1,4]], [38], [0], [38], [1.09],
//   [LR exp(0.3)], [[1,4]], [50], [0], [50], [2.31e-1],
  
//   [LR const(0.1)], [[1,1]], [5300], [0], [5300], [1.28],
//   [LR exp(0.5)], [[1,1]], [36], [0], [36], [1.07],
//   [LR exp(0.3)], [[1,1]], [45], [0], [45], [2.39e-1],
  
//   // Armijo data
//   [Armijo GD], [[0,0]], [895], [14935], [895], [1.93e-1],
//   [Armijo GD], [[1,4]], [4081], [67705], [4081], [3.28e-1],
//   [Armijo GD], [[1,1]], [314], [5234], [314], [1.27e-1],
  
//   // Dichotomy data
//   [Dichotomy GD], [[0,0]], [10002], [608011], [10002], [8.17e-2],
//   [Dichotomy GD], [[1,4]], [10002], [608011], [10002], [1.76e-1],
//   [Dichotomy GD], [[1,1]], [10002], [608359], [10002], [2.62e-1],
  
//   // Scipy Wolfe data
//   [Scipy Wolfe GD], [[0,0]], [1880], [37471], [6543], [1.66e-1],
//   [Scipy Wolfe GD], [[1,4]], [2148], [43464], [7490], [7.39e-2],
//   [Scipy Wolfe GD], [[1,1]], [7222], [146795], [25245], [1.92e-1],
// )

// #image("noisy f4.png")

// #table(
//   columns: (auto, auto, auto, auto, auto, auto),
//   align: center + horizon,
//   [*Method*], [*x0*], [*Iterations*], [*Function Evals*], [*Gradient Evals*], [*Error*],
  
//   // LR scheduling data
//   [LR const(0.1)], [[0,0]], [332], [0], [332], [3.36e-1],
//   [LR exp(0.5)], [[0,0]], [8], [0], [8], [2.80e+117],
//   [LR exp(0.3)], [[0,0]], [7], [0], [7], [2.14e+118],
  
//   [LR const(0.1)], [[1,4]], [3563], [0], [3563], [1.30],
//   [LR exp(0.5)], [[1,4]], [38], [0], [38], [1.87],
//   [LR exp(0.3)], [[1,4]], [50], [0], [50], [8.60e-1],
  
//   [LR const(0.1)], [[1,1]], [2157], [0], [2157], [1.32],
//   [LR exp(0.5)], [[1,1]], [36], [0], [36], [1.18],
//   [LR exp(0.3)], [[1,1]], [45], [0], [45], [1.72e-1],
  
//   // Armijo data
//   [Armijo GD], [[0,0]], [5825], [59911], [5825], [4.10e-1],
//   [Armijo GD], [[1,4]], [473], [4699], [473], [8.87e-1],
//   [Armijo GD], [[1,1]], [3926], [40396], [3926], [6.06e-1],
  
//   // Dichotomy data
//   [Dichotomy GD], [[0,0]], [10002], [608861], [10002], [1.07],
//   [Dichotomy GD], [[1,4]], [10002], [609177], [10002], [7.54e-1],
//   [Dichotomy GD], [[1,1]], [10002], [609113], [10002], [9.62e-1],
  
//   // Scipy Wolfe data
//   [Scipy Wolfe GD], [[0,0]], [10002], [169263], [35263], [1.91e-1],
//   [Scipy Wolfe GD], [[1,4]], [2278], [37890], [8043], [1.85e-1],
//   [Scipy Wolfe GD], [[1,1]], [6720], [112130], [23472], [6.37e-1],
// )
